version: '2'
services:
  zookeeper:
    image: 'bitnami/zookeeper:3.7.0'
    hostname: myzookeeper
    restart: on-failure
    user: root
    environment:
      ALLOW_ANONYMOUS_LOGIN: 'yes'
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    ports:
      - 2181:2181
    networks:
      - my_persistent_network
  kafka:
    image: wurstmeister/kafka
    container_name: kafka
    restart: on-failure
    user: root
    depends_on:
      - zookeeper
    ports:
      - 9092:9092
    networks:
      - my_persistent_network
    environment:
      KAFKA_CREATE_TOPICS: "Topic1:1:1,Topic2:1:1:compact"
      KAFKA_ADVERTISED_LISTENERS: INSIDE://kafka:9092,OUTSIDE://localhost:9093
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INSIDE:PLAINTEXT,OUTSIDE:PLAINTEXT
      KAFKA_LISTENERS: INSIDE://0.0.0.0:9092,OUTSIDE://0.0.0.0:9093
      KAFKA_INTER_BROKER_LISTENER_NAME: INSIDE
      KAFKA_ZOOKEEPER_CONNECT: myzookeeper:2181
  # version control for nifi flows
  registry:
      hostname: myregistry
      container_name: registry_container_persistent
      image: 'apache/nifi-registry:1.15.0'  # latest image as of 2021-11-09.
      restart: on-failure
      ports:
          - "18080:18080"
      environment:
          - LOG_LEVEL=INFO
          - NIFI_REGISTRY_DB_DIR=/opt/nifi-registry/nifi-registry-current/database
          - NIFI_REGISTRY_FLOW_PROVIDER=file
          - NIFI_REGISTRY_FLOW_STORAGE_DIR=/opt/nifi-registry/nifi-registry-current/flow_storage
      volumes:
          - ./nifi_registry/database:/opt/nifi-registry/nifi-registry-current/database
          - ./nifi_registry/flow_storage:/opt/nifi-registry/nifi-registry-current/flow_storage
      networks:
          - my_persistent_network
  # data extraction, transformation and load service
  nifi:
      hostname: mynifi
      container_name: nifi_container_persistent
      image: 'apache/nifi:1.14.0'  # latest image as of 2021-11-09.
      restart: on-failure
      ports:
          - '8091:8080'
      environment:
          - NIFI_WEB_HTTP_PORT=8080
          - NIFI_CLUSTER_IS_NODE=true
          - NIFI_CLUSTER_NODE_PROTOCOL_PORT=8082
          - NIFI_ZK_CONNECT_STRING=myzookeeper:2181
          - NIFI_ELECTION_MAX_WAIT=30 sec
          - NIFI_SENSITIVE_PROPS_KEY='12345678901234567890A'
      healthcheck:
          test: "${DOCKER_HEALTHCHECK_TEST:-curl localhost:8091/nifi/}"
          interval: "60s"
          timeout: "3s"
          start_period: "5s"
          retries: 5
      volumes:
          - ./nifi/database_repository:/opt/nifi/nifi-current/database_repository
          - ./nifi/flowfile_repository:/opt/nifi/nifi-current/flowfile_repository
          - ./nifi/content_repository:/opt/nifi/nifi-current/content_repository
          - ./nifi/provenance_repository:/opt/nifi/nifi-current/provenance_repository
          - ./nifi/state:/opt/nifi/nifi-current/state
          - ./nifi/logs:/opt/nifi/nifi-current/logs
          # uncomment the next line after copying the /conf directory from the container to your local directory to persist NiFi flows
          - ./nifi/conf:/opt/nifi/nifi-current/conf
      networks:
          - my_persistent_network
  mqtt5:
    image: eclipse-mosquitto
    hostname: mosquitto
    container_name: mqtt5
    ports:
      - 8883:8883  
      # - 9001:9001
    user: 1000:1000
    restart: unless-stopped
    volumes:
      - ./mosquitto:/mosquitto  
      - ./mosquitto/config/mosquitto.conf:/mosquitto/config/mosquitto.conf
      - ./mosquitto/certs:/mosquitto/config/certs
      - ./mosquitto/config:/mosquitto/config
      - ./mosquitto/data:/mosquitto/data
      - ./mosquitto/log:/mosquitto/log
    networks:
        - webnet
  kafdrop:
    image: obsidiandynamics/kafdrop
    restart: "no"
    ports:
      - "9000:9000"
    environment:
      KAFKA_BROKERCONNECT: "kafka:9092"
    depends_on:
      - "kafka"
    networks:
      - my_persistent_network
  namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    container_name: namenode
    restart: always
    ports:
      - 9870:9870
      - 9010:9000
    volumes:
      - "./hadoop_namenode:/hadoop/dfs/name"
    environment:
      - CLUSTER_NAME=test
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
    env_file:
      - ./hadoop.env

  datanode:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: datanode
    restart: always
    volumes:
      - "./hadoop_datanode:/hadoop/dfs/data"
    environment:
      SERVICE_PRECONDITION: "namenode:9870"
      CORE_CONF_fs_defaultFS: hdfs://namenode:9000
    ports:
      - "9864:9864"
    env_file:
      - ./hadoop.env

  resourcemanager:
    image: bde2020/hadoop-resourcemanager:2.0.0-hadoop3.2.1-java8
    container_name: resourcemanager
    restart: always
    environment:
      SERVICE_PRECONDITION: "namenode:9000 namenode:9870 datanode:9864"
    env_file:
      - ./hadoop.env

  nodemanager1:
    image: bde2020/hadoop-nodemanager:2.0.0-hadoop3.2.1-java8
    container_name: nodemanager
    restart: always
    environment:
      SERVICE_PRECONDITION: "namenode:9000 namenode:9870 datanode:9864 resourcemanager:8088"
    env_file:
      - ./hadoop.env

  historyserver:
    image: bde2020/hadoop-historyserver:2.0.0-hadoop3.2.1-java8
    container_name: historyserver
    restart: always
    environment:
      SERVICE_PRECONDITION: "namenode:9000 namenode:9870 datanode:9864 resourcemanager:8088"
    volumes:
      - "./hadoop_historyserver:/hadoop/yarn/timeline"
    env_file:
      - ./hadoop.env

  spark-master:
    image: bde2020/spark-master:3.0.0-hadoop3.2
    container_name: spark-master
    depends_on:
      - namenode
      - datanode
    networks:
      - my_persistent_network
    ports:
      - "8080:8083"
      - "7077:7077"
    environment:
      - INIT_DAEMON_STEP=setup_spark
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
    volumes:
      - "./process_job_lib:/process_job_lib"

  spark-worker-1:
    image: bde2020/spark-worker:3.0.0-hadoop3.2
    container_name: spark-worker-1
    depends_on:
      - spark-master
    ports:
      - "8081:8081"
    environment:
      - "SPARK_MASTER=spark://spark-master:7077"
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000

  presto-coordinator:
    image: shawnzhu/prestodb:0.181
    container_name: presto-coordinator
    ports:
      - "8089:8089"


networks:
  webnet:
  my_persistent_network:
    driver: bridge